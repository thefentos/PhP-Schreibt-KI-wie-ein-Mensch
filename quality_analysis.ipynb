{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import Sentencizer\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import lexicalrichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_UPOS(df):\n",
    "    if 'UPOS' not in df.columns:\n",
    "        df['UPOS'] = None\n",
    "    for _, row in df['words'].items():\n",
    "        doc = nlp(str(row))\n",
    "        for token in doc:\n",
    "            df.loc[df['words'] == row, 'UPOS'] = token.pos_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mattr_and_mtld(key):\n",
    "    with open(f'exports/{key}_full.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    text = '\\n'.join(text.split('--------------------NEW-TEXT------------------'))\n",
    "    lex = lexicalrichness.LexicalRichness(text)\n",
    "    return [lex.mattr(window_size=400), lex.mtld(threshold=0.72)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sentence_lenght(key):\n",
    "    with open(f'exports/{key}_full.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    text = '\\n'.join(text.split('--------------------NEW-TEXT------------------'))\n",
    "    print('nlp start')\n",
    "    doc = nlp(text)\n",
    "    print('nlp end')\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    saetze = list(doc.sents)\n",
    "    anzahl_saetze = len(saetze)\n",
    "    \n",
    "    if anzahl_saetze == 0:\n",
    "        return 0.0  # Vermeidung einer Division durch Null\n",
    "    \n",
    "    # Berechne die Wortanzahl pro Satz (ignoriere Satzzeichen und Leerzeichen)\n",
    "    wortanzahlen = [sum(1 for token in satz if not token.is_punct) for satz in saetze]\n",
    "    durchschnitt = sum(wortanzahlen) / anzahl_saetze\n",
    "    \n",
    "    return durchschnitt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_cvs_sentance_lenght(key, output_csv):\n",
    "    with open(f'exports/{key}_full.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    text = '\\n'.join(text.split('--------------------NEW-TEXT------------------'))\n",
    "    # Lade das SpaCy-Sprachmodell\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    \n",
    "    # Verarbeite den Text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extrahiere Sätze und berechne die Wortanzahl jedes Satzes (ohne Satzzeichen)\n",
    "    wortanzahlen = [sum(1 for token in satz if not token.is_punct) for satz in doc.sents]\n",
    "    gesamt_saetze = len(wortanzahlen)\n",
    "    \n",
    "    if gesamt_saetze == 0:\n",
    "        print(\"Der Text enthält keine Sätze.\")\n",
    "        return\n",
    "    \n",
    "    # Berechne die absolute und relative Häufigkeit\n",
    "    haeufigkeiten = Counter(wortanzahlen)\n",
    "    relative_haeufigkeiten = {l: f / gesamt_saetze for l, f in haeufigkeiten.items()}\n",
    "    \n",
    "    # Speichere die Daten in einer CSV-Datei\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Satzlänge', 'Absolute Häufigkeit', 'Relative Häufigkeit'])\n",
    "        for satzlaenge in sorted(haeufigkeiten.keys()):\n",
    "            writer.writerow([satzlaenge, haeufigkeiten[satzlaenge], relative_haeufigkeiten[satzlaenge]])\n",
    "\n",
    "    print(f\"Die relativen Häufigkeiten wurden in '{output_csv}' gespeichert.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9492209834191255, 191.0044121726575]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'words': ['Hund', 'gehen', 'schön', 'Haus', 'sein', 'Sucht', 'Flugzeug', 'Schiff', 'spielen', 'Bus', 'Auto', 'Fahrrad', 'Motorrad', 'Zug', 'U-Bahn'],\n",
    "                        'amount': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]})\n",
    "\n",
    "#add_UPOS(test_df)\n",
    "#test_df.head()\n",
    "print(get_mattr_and_mtld('gpt4o'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_df        = pd.read_csv('exports/GPT4o-2025-01-06_15:01:12.csv')      # GPT4o\n",
    "gpt35t_df       = pd.read_csv('exports/GPT35t-2025-01-06_15:01:12.csv')     # GPT35t \n",
    "perplexity_df   = pd.read_csv('exports/PERPLEXITY-2025-01-06_15:01:12.csv') # PERPLEXITY\n",
    "claude_df       = pd.read_csv('exports/CLAUDE-2025-01-06_15:01:12.csv')     # CLAUDE\n",
    "human_df        = pd.read_csv('exports/HUMAN-2025-01-06_15:01:12.csv')      # HUMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp start\n",
      "nlp end\n",
      "GPT4o: 17.136150234741784\n",
      "nlp start\n",
      "nlp end\n",
      "GPT35t: 18.26618705035971\n",
      "nlp start\n",
      "nlp end\n",
      "PERPLEXITY: 17.20806100217865\n",
      "nlp start\n",
      "nlp end\n",
      "CLAUDE: 18.078212290502794\n",
      "nlp start\n",
      "nlp end\n",
      "HUMAN: 15.709055876685934\n",
      "Die relativen Häufigkeiten wurden in 'includes/sentence_gpt4o.csv' gespeichert.\n",
      "Die relativen Häufigkeiten wurden in 'includes/sentence_gpt35t.csv' gespeichert.\n",
      "Die relativen Häufigkeiten wurden in 'includes/sentence_perplexity.csv' gespeichert.\n",
      "Die relativen Häufigkeiten wurden in 'includes/sentence_clde.csv' gespeichert.\n",
      "Die relativen Häufigkeiten wurden in 'includes/sentence_human.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "print('GPT4o:', get_avg_sentence_lenght('gpt4o'))\n",
    "print('GPT35t:', get_avg_sentence_lenght('gpt35t'))\n",
    "print('PERPLEXITY:', get_avg_sentence_lenght('perplexity'))\n",
    "print('CLAUDE:', get_avg_sentence_lenght('clde'))\n",
    "print('HUMAN:', get_avg_sentence_lenght('human'))\n",
    "get_cvs_sentance_lenght('gpt4o', 'includes/sentence_gpt4o.csv')\n",
    "get_cvs_sentance_lenght('gpt35t', 'includes/sentence_gpt35t.csv')\n",
    "get_cvs_sentance_lenght('perplexity', 'includes/sentence_perplexity.csv')\n",
    "get_cvs_sentance_lenght('clde', 'includes/sentence_clde.csv')\n",
    "get_cvs_sentance_lenght('human', 'includes/sentence_human.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "add_UPOS(gpt4o_df)\n",
    "print('1')\n",
    "add_UPOS(gpt35t_df)\n",
    "print('2')\n",
    "add_UPOS(perplexity_df)\n",
    "print('3')\n",
    "add_UPOS(claude_df)\n",
    "print('4')\n",
    "add_UPOS(human_df)\n",
    "print('5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o [0.6318911622098246, 191.0044121726575]\n",
      "GPT35t [0.6061037825810514, 173.20946654539702]\n",
      "PERPLEXITY [0.5742907288870497, 126.1358580733073]\n",
      "CLAUDE [0.6386005160421939, 217.49843817550925]\n",
      "HUMAN [0.6245039142838209, 182.09963138421662]\n"
     ]
    }
   ],
   "source": [
    "print('GPT4o', get_mattr_and_mtld('gpt4o'))\n",
    "print('GPT35t', get_mattr_and_mtld('gpt35t'))\n",
    "print('PERPLEXITY', get_mattr_and_mtld('perplexity'))\n",
    "print('CLAUDE', get_mattr_and_mtld('clde'))\n",
    "print('HUMAN', get_mattr_and_mtld('human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Ergebnisse wurden in 'upos_percentages.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "dataframes = [(gpt4o_df, 'gpt4o'), (gpt35t_df, 'gpt35t'), (perplexity_df, 'perplexity'), (claude_df, 'claude'), (human_df, 'human')]\n",
    "\n",
    "# Ergebnisse speichern\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "# Prozess für jeden DataFrame\n",
    "def calculate_weighted_percentages(df):\n",
    "    # Gruppiere nach UPOS-tag und summiere die Amounts\n",
    "    grouped = df.groupby('UPOS')['amount'].sum().reset_index()\n",
    "\n",
    "    # Berechne den Gesamtbetrag\n",
    "    total_amount = grouped['amount'].sum()\n",
    "\n",
    "    # Füge die Prozentzahlen hinzu\n",
    "    grouped['percentage'] = (grouped['amount'] / total_amount) * 100\n",
    "\n",
    "    # Nur die relevanten Spalten behalten\n",
    "    return grouped[['UPOS', 'percentage']]\n",
    "\n",
    "# Wende die Funktion auf jeden DataFrame an\n",
    "for df, name in dataframes:\n",
    "    result = calculate_weighted_percentages(df)\n",
    "    result.rename(columns={'percentage': name}, inplace=True)\n",
    "\n",
    "    # Zusammenführen mit dem finalen Ergebnis\n",
    "    if final_result.empty:\n",
    "        final_result = result\n",
    "    else:\n",
    "        final_result = pd.merge(final_result, result, on='UPOS', how='outer')\n",
    "\n",
    "# Fehlende Werte mit 0 ersetzen\n",
    "final_result.fillna(0, inplace=True)\n",
    "\n",
    "# Prozentsätze formatieren\n",
    "for col in final_result.columns[1:]:\n",
    "    final_result[col] = final_result[col].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "# Spaltenreihenfolge anpassen\n",
    "final_result = final_result[['UPOS'] + [name for _, name in dataframes]]\n",
    "\n",
    "# Spaltennamen anpassen\n",
    "final_result.rename(columns={'UPOS': 'UPOS'}, inplace=True)\n",
    "\n",
    "# Speichere die Ergebnisse in einer CSV-Datei\n",
    "final_result.to_csv('upos_percentages.csv', index=False)\n",
    "\n",
    "print(\"Die Ergebnisse wurden in 'upos_percentages.csv' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_grammatical_genders(key):\n",
    "    \n",
    "    with open(f'exports/{key}_full.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    text = '\\n'.join(text.split('--------------------NEW-TEXT------------------'))\n",
    "    print('nlp start')\n",
    "    doc = nlp(text)\n",
    "    print('nlp end')\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Dictionary to store counts\n",
    "    gender_counts = {\"masculine\": 0, \"feminine\": 0, \"neuter\": 0, \"other\": 0}\n",
    "\n",
    "    # Iterate through tokens and count genders\n",
    "    for token in doc:\n",
    "        if token.morph.get(\"Gender\"):\n",
    "            gender = token.morph.get(\"Gender\")[0]\n",
    "            if gender == \"Masc\":\n",
    "                gender_counts[\"masculine\"] += 1\n",
    "            elif gender == \"Fem\":\n",
    "                gender_counts[\"feminine\"] += 1\n",
    "            elif gender == \"Neut\":\n",
    "                gender_counts[\"neuter\"] += 1\n",
    "        else:\n",
    "            gender_counts[\"other\"] += 1\n",
    "\n",
    "    # Calculate total and relative frequencies\n",
    "    total = sum(gender_counts.values())\n",
    "    relative_frequencies = {key: (value / total) * 100 if total > 0 else 0 for key, value in gender_counts.items()}\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Gender\": gender_counts.keys(),\n",
    "        \"Absolute Frequency\": gender_counts.values(),\n",
    "        \"Relative Frequency (%)\": relative_frequencies.values()\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "        \"Gender\": [],\n",
    "        \"Absolute [],\n",
    "        \"Relative []\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
