{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(key):\n",
    "    with open(f'exports/{key}_full.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    return text.split('--------------------NEW-TEXT------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "emotion_model = pipeline(\"text-classification\", model=\"visegradmedia-emotion/Emotion_RoBERTa_german6_v7\", top_k=None, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_s = 'Freude'\n",
    "fear_s = 'Angst'\n",
    "anger_s = 'ich bin Wütend'\n",
    "sadness_s = 'Traurigkeit'\n",
    "disgust_s = 'Ekel'\n",
    "none_s = 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOY:  [[{'label': 'LABEL_4', 'score': 0.977495014667511}, {'label': 'LABEL_5', 'score': 0.008196782320737839}, {'label': 'LABEL_0', 'score': 0.007576040457934141}, {'label': 'LABEL_3', 'score': 0.006525225006043911}, {'label': 'LABEL_2', 'score': 0.003997484687715769}, {'label': 'LABEL_1', 'score': 0.001897070906125009}]]\n",
      "FEAR:  [[{'label': 'LABEL_1', 'score': 0.9911304712295532}, {'label': 'LABEL_5', 'score': 0.007378977723419666}, {'label': 'LABEL_0', 'score': 0.00580309284850955}, {'label': 'LABEL_3', 'score': 0.004539343062788248}, {'label': 'LABEL_4', 'score': 0.0035139520186930895}, {'label': 'LABEL_2', 'score': 0.001936954678967595}]]\n",
      "ANGER:  [[{'label': 'LABEL_0', 'score': 0.9243784546852112}, {'label': 'LABEL_2', 'score': 0.04132488742470741}, {'label': 'LABEL_5', 'score': 0.027078239247202873}, {'label': 'LABEL_3', 'score': 0.015537630766630173}, {'label': 'LABEL_4', 'score': 0.003564917016774416}, {'label': 'LABEL_1', 'score': 0.0031422206666320562}]]\n",
      "SADNESS:  [[{'label': 'LABEL_3', 'score': 0.9869602918624878}, {'label': 'LABEL_5', 'score': 0.007522836327552795}, {'label': 'LABEL_4', 'score': 0.006339768413454294}, {'label': 'LABEL_0', 'score': 0.005944435019046068}, {'label': 'LABEL_2', 'score': 0.005804221145808697}, {'label': 'LABEL_1', 'score': 0.001605546218343079}]]\n",
      "DISGUST:  [[{'label': 'LABEL_2', 'score': 0.7626466155052185}, {'label': 'LABEL_5', 'score': 0.05316323786973953}, {'label': 'LABEL_0', 'score': 0.027762215584516525}, {'label': 'LABEL_3', 'score': 0.0077792080119252205}, {'label': 'LABEL_4', 'score': 0.006714502349495888}, {'label': 'LABEL_1', 'score': 7.415327854687348e-05}]]\n",
      "NONE:  [[{'label': 'LABEL_5', 'score': 0.8862751126289368}, {'label': 'LABEL_0', 'score': 0.06103196367621422}, {'label': 'LABEL_4', 'score': 0.022147109732031822}, {'label': 'LABEL_3', 'score': 0.004783276002854109}, {'label': 'LABEL_2', 'score': 0.002905550878494978}, {'label': 'LABEL_1', 'score': 0.0005359503556974232}]]\n"
     ]
    }
   ],
   "source": [
    "print('JOY: ', emotion_model(joy_s))\n",
    "print('FEAR: ', emotion_model(fear_s))\n",
    "print('ANGER: ', emotion_model(anger_s))\n",
    "print('SADNESS: ', emotion_model(sadness_s))\n",
    "print('DISGUST: ', emotion_model(disgust_s))\n",
    "print('NONE: ', emotion_model(none_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label_map = {\n",
    "    'LABEL_0': 'anger',\n",
    "    'LABEL_1': 'fear',\n",
    "    'LABEL_2': 'disgust',\n",
    "    'LABEL_3': 'sadness',\n",
    "    'LABEL_4': 'joy',\n",
    "    'LABEL_5': 'none'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, max_length):\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_emotions(text_lists, max_length=512, normalization_size=20):\n",
    "    \"\"\"\n",
    "    Analysiert mehrere Listen von Texten auf Emotionen, unterteilt lange Texte in Abschnitte\n",
    "    und normalisiert die Ergebnisse auf eine feste Anzahl von Texten pro Liste.\n",
    "\n",
    "    :param text_lists: Liste von Listen mit Texten zur Analyse\n",
    "    :param emotion_model: Funktion oder Modell zur Emotionserkennung\n",
    "    :param emotion_label_map: Wörterbuch zur Übersetzung von Labels in lesbare Emotionsnamen\n",
    "    :param max_length: Maximale Länge eines Textabschnitts\n",
    "    :param normalization_size: Anzahl der Zeilen, auf die die Ergebnisse normalisiert werden\n",
    "    :return: DataFrame mit Emotionswerten pro Textliste\n",
    "    \"\"\"\n",
    "    def split_text_into_chunks(text, max_length):\n",
    "        \"\"\"Hilfsfunktion, um Texte in Abschnitte aufzuteilen.\"\"\"\n",
    "        chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "        chunk_lenghs = [len(chunk.split(' ')) for chunk in chunks]\n",
    "        return chunks, chunk_lenghs\n",
    "\n",
    "    # Liste zur Speicherung der Durchschnittswerte für jede Textliste\n",
    "    averages = []\n",
    "\n",
    "    for texts in text_lists:\n",
    "        # Liste zur Speicherung von Emotionsergebnissen\n",
    "        emotion_results = []\n",
    "        \n",
    "\n",
    "        for text in texts:\n",
    "            # Text in Abschnitte aufteilen, falls er zu lang ist\n",
    "            chunks = split_text_into_chunks(text, max_length)\n",
    "            chunk_lenghs = chunks[1]\n",
    "            chunks = chunks[0]\n",
    "\n",
    "            for chunk in chunks:\n",
    "                # Emotionen für jeden Abschnitt analysieren\n",
    "                emotions = emotion_model(chunk)\n",
    "\n",
    "                # Extrahiere Ergebnisse und speichere sie in einer Liste von Dictionnaries\n",
    "                for item in emotions:\n",
    "                    for item in item:\n",
    "                        emotion_label = item['label']\n",
    "                        emotion_score = item['score']\n",
    "\n",
    "                        # Übersetzung des Labels in die Emotion\n",
    "                        emotion_name = emotion_label_map.get(emotion_label, emotion_label)  # Fallback, falls Label fehlt\n",
    "\n",
    "                        emotion_results.append({emotion_name: emotion_score})\n",
    "\n",
    "        # DataFrame aus den Ergebnissen erstellen\n",
    "        emotions_df = pd.DataFrame(emotion_results)\n",
    "\n",
    "        # Fehlende Werte mit 0 ersetzen\n",
    "        emotions_df = emotions_df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "        # Durchschnitt der Emotionen berechnen\n",
    "        average_emotions = emotions_df.mean().to_dict()\n",
    "        averages.append(average_emotions)\n",
    "\n",
    "    # Endgültiges DataFrame erstellen, mit einer Zeile pro Textliste\n",
    "    final_df = pd.DataFrame(averages, index=['gpt4o', 'gpt35t', 'perplexity', 'clde', 'human'])\n",
    "\n",
    "    # Fehlende Spalten mit 0 auffüllen (falls nicht alle Labels in allen Textlisten vorkommen)\n",
    "    final_df = final_df.fillna(0)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotions(text_lists):\n",
    "\n",
    "    def get_chunks(text):\n",
    "        chunks = [text[i:i + 512] for i in range(0, len(text), 512)]\n",
    "        chunk_lenghs = [len(chunk) for chunk in chunks]\n",
    "        #print(chunks)\n",
    "        #print(chunk_lenghs)\n",
    "        return chunks, chunk_lenghs\n",
    "\n",
    "\n",
    "    def get_w_avg(column, weights):\n",
    "        column = np.array(column, dtype='float')\n",
    "        weights = np.array(weights, dtype='float')\n",
    "        return np.average(column, weights=weights)\n",
    "\n",
    "\n",
    "    g_df = pd.DataFrame(\n",
    "        columns=['anger', 'fear', 'disgust', 'sadness', 'joy', 'none'],\n",
    "    )\n",
    "\n",
    "    for modell in text_lists:\n",
    "\n",
    "        chunk_lens = []\n",
    "        chunks     = []\n",
    "\n",
    "        for text in modell:\n",
    "            i_chunk = get_chunks(text)\n",
    "            chunk_lens.extend(i_chunk[1])\n",
    "            chunks.extend(i_chunk[0])\n",
    "\n",
    "        #print(chunks)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            columns=['anger', 'fear', 'disgust', 'sadness', 'joy', 'none'],\n",
    "        )\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_em = {}\n",
    "            #print(chunk)\n",
    "            chunk_emotions = emotion_model(inputs=chunk)\n",
    "            for i_emotion in chunk_emotions:\n",
    "                for emotion in i_emotion:\n",
    "                    emotion_label = emotion_label_map[emotion['label']]\n",
    "                    emotion_score = emotion['score']\n",
    "                    chunk_em[emotion_label]=emotion_score\n",
    "\n",
    "            #print(chunk_em)\n",
    "            \n",
    "\n",
    "            \n",
    "            df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n",
    "            \n",
    "\n",
    "        \n",
    "        print(len(df['anger'].values.tolist()), len(chunk_lens))\n",
    "        \n",
    "        final = {\n",
    "            \"anger\": get_w_avg(df['anger'].values.tolist(), chunk_lens),\n",
    "            \"fear\": get_w_avg(df['fear'].values.tolist(), chunk_lens),\n",
    "            \"disgust\":get_w_avg(df['disgust'].values.tolist(), chunk_lens),\n",
    "            \"sadness\":get_w_avg(df['sadness'].values.tolist(), chunk_lens),\n",
    "            \"joy\":get_w_avg(df['joy'].values.tolist(), chunk_lens),\n",
    "            \"none\":get_w_avg(df['none'].values.tolist(), chunk_lens),\n",
    "        }\n",
    "\n",
    "\n",
    "        g_df = pd.concat([g_df, pd.DataFrame([final])], ignore_index=True)\n",
    "    \n",
    "    return g_df\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lists = [get_texts('gpt4o'), get_texts('gpt35t'), get_texts('perplexity'), get_texts('clde'), get_texts('human')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analyze_emotions(text_lists)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  g_df = pd.concat([g_df, pd.DataFrame([final])], ignore_index=True)\n",
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/r_qwq8cj7ds1wzjt6wsqm2g80000gn/T/ipykernel_49542/3668719936.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([chunk_em])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 223\n"
     ]
    }
   ],
   "source": [
    "analyze_emotions(text_lists).to_csv('includes/emotions_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "         LABEL_0    LABEL_1     LABEL_2     LABEL_3     LABEL_4     LABEL_5\n",
    "gpt4o\n",
    "gpt35t\n",
    "perplexity\n",
    "clde\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
